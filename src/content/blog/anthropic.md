---
title: "ü§ñ My Anthropic Rollercoaster"
date: "2026-02-28"
slug: anthropic
lang: en
tags: [tech, leftism, courage]
excerpt: "I went from dismissing Anthropic as a woke EA vanity project to placing them second only to xAI-and then Trump banned them from every federal agency in America overnight."
featuredImage: "/assets/anthropic.png"
pairedSlug: anthropic-ko
---

I went from dismissing Anthropic as a woke EA vanity project to placing them second only to xAI-and then Trump banned them from every federal agency in America overnight. That is quite a trajectory for a single company to trace in the span of a few months.

### Phase One: The Dismissal

Let me be honest about where I started. When Anthropic first crossed my radar, I filed them under ‚Äúcompanies I don‚Äôt need to take seriously.‚Äù The effective altruism branding was a red flag‚Äîa secular religion that dresses up utilitarian calculation in the language of moral urgency while its adherents quietly accumulate influence and capital. The founders came out of OpenAI with a lot of philosophical noise about ‚ÄúAI safety‚Äù and ‚Äúresponsible scaling,‚Äù which to my ear sounded like Silicon Valley progressivism with extra steps.

My read was that they would end up a distant third or fourth behind xAI, Google, OpenAI, and Meta. A niche player for academics and NGOs. I moved on.

### Phase Two: I Had to Eat My Words

Then Claude showed up in my actual workflow‚Äîand I had to pay attention.

Claude 4.6 Sonnet and Opus genuinely impressed me. Not in a ‚Äúnice demo‚Äù way, but in a ‚ÄúI am using this for real work and it is consistently good‚Äù way. Their strategic positioning also started to make more sense: focused primarily on coding and enterprise use cases, not chasing image generation or AGI hype. It was a disciplined product strategy, and it was paying off.

At one point I found myself paying for exactly two AI subscriptions: Claude and Grok. That is the most selective I have ever been, and Anthropic had earned one of those two slots. Their speed of execution matched it-CoWork, memory features, steady model improvements. I placed them firmly in second place behind xAI.

I was genuinely warming to them.

### Phase Three: The Fall

Then came the news last night.

Trump posted on Truth Social:

> ‚ÄúTHE UNITED STATES OF AMERICA WILL NEVER ALLOW A RADICAL LEFT, WOKE COMPANY TO DICTATE HOW OUR GREAT MILITARY FIGHTS AND WINS WARS! That decision belongs to YOUR COMMANDER-IN-CHIEF, and the tremendous leaders I appoint to run our Military. The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution. Their selfishness is putting AMERICAN LIVES at risk, our Troops in danger, and our National Security in JEOPARDY. Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic‚Äôs technology.‚Äù

And Secretary Hegseth followed with his own statement:

> ‚ÄúAnthropic delivered a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon. Our position has never wavered and will never waver: the Department of War must have full, unrestricted access to Anthropic‚Äôs models for every LAWFUL purpose in defense of the Republic. Instead, Anthropic and its CEO Dario Amodei have chosen duplicity. Cloaked in the sanctimonious rhetoric of ‚Äòeffective altruism,‚Äô they have attempted to strong-arm the United States military into submission‚Äîa cowardly act of corporate virtue-signaling that places Silicon Valley ideology above American lives.‚Äù

That is devastating. Not just politically‚Äîstrategically, commercially, reputationally.

### The ‚ÄúStolen Land‚Äù Problem

What makes this more than a contract dispute is the ideological layer underneath it. The allegation is not merely that Anthropic tried to impose its Terms of Service on the Pentagon‚Äîit is that Claude was trained with embedded political assumptions. One of the claims circulating is that the model was trained to treat America as a nation built on ‚Äústolen land.‚Äù

Think carefully about what that means if true.

If you encode into an AI system the belief that every American is, in some foundational moral sense, a criminal occupying territory they have no right to‚Äîyou have not built a helpful assistant. You have built a philosophical adversary. The ‚Äúethics‚Äù of such a system would be structurally hostile to the civilization it serves. That is not a theoretical concern. That is a design feature with consequences.

Effective altruism claims to calculate moral goodness with rigorous neutrality, but it launders specific ideological priors through the language of reason and charity‚Äîexactly the syndrome [Solzhenitsyn diagnosed in Western elites](/solzhenitsyn/) half a century before Silicon Valley existed. The ‚Äústolen land‚Äù premise is not a conclusion anyone arrived at through neutral ethical calculation. It is a political position‚Äîone that millions of Americans, including most of the people who would actually deploy this AI in defense contexts, find not just wrong but offensive.

When your model‚Äôs ethics are downstream of progressive academic ideology, you do not get to act surprised when the government calls it incompatible with national defense.

### What I Think Now

I still think the Claude models are technically excellent. That has not changed. But technical excellence bundled with adversarial ideology is not a product I can fully trust‚Äîand apparently neither can the U.S. federal government.

Anthropic built something genuinely impressive and then undermined it by trying to impose their worldview on institutions that were paying them for capability, not catechism. There is a six-month phase-out period, a threat of civil and criminal consequences if Anthropic is uncooperative, and a permanent relationship rupture with the Pentagon.

For a company that positioned itself as the ‚Äúresponsible‚Äù AI lab‚Äîthe adults in the room‚Äîthis is an extraordinary self-own. You do not get to be the responsible choice while strong-arming the military with your terms of service.

### Peak Clown World

I have said this before and I will say it again: we are living through a moment where the people who lecture most loudly about safety and ethics tend to be the ones creating the most dangerous ideological situations. Effective altruism is exhibit A‚Äîa movement that talks endlessly about existential risk while training AI on premises that existentially delegitimize the civilization it operates in.

It is clown world. The most ‚Äúsafety-conscious‚Äù AI lab just got banned from every federal agency in the United States because it tried to override the Commander-in-Chief.

### Who Governs the Military?

This gets to the core of the issue more than any debate about specific terms. Do you believe in democracy? Should our military be regulated by elected leaders, or by corporate executives?

Seemingly innocuous terms like ‚Äúyou cannot target innocent civilians‚Äù are actually moral minefields. Who defines civilian? What makes someone innocent? What is the difference between a target and collateral damage? Existing law and policy‚Äîhammered out over decades by generals, lawyers, and elected governments‚Äîhas very clear answers to these questions. Unelected corporations managing profits and PR will often have a very different answer. And their answer will be shaped by whoever is running the company, whatever their ideological priors happen to be.

Imagine a missile company tried to enforce this. ‚ÄúOur product cannot be used to target innocent civilians. We will shut off access if elected leaders break our terms.‚Äù Sounds reasonable, right? Look harder. In addition to the definitional problems above, consider:

- What level of classified information does the corporation need to make these determinations? How much leverage does that give them to demand more?
- What if a President merely *threatens* a dictator‚ÄîMadman Theory, mutual assured destruction? Is the threat empty because the dictator knows the corporate executives can cut off the military? Does the threat alone trigger the cutoff? And how might that calculus shift depending on whether the executive happens to like the dictator or dislike the President?
- At what confidence threshold does the cutoff trigger, both on paper and in practice?

The fact that this is a debate about AI rather than missiles does not change the underlying arithmetic. The same problems apply to surveillance systems, autonomous weapons, and every other dual-use capability that matters. ‚ÄúBut they will have cutouts for purely defensive use!‚Äù‚Äîfine, but what is autonomous? What is defensive? What about defending an asset during an offensive action, or parking a carrier group off the coast of a nation that considers the carrier group itself to be an act of aggression?

Palmer Luckey‚Äîthe founder of Oculus, now building defence technology through Anduril‚Äîput his finger on what is really happening here. This fight is not about autonomous weapons. It is about democratic control of the military, and by extension the nation. [Killer robots](/anthropic-war/) are coming. When they arrive, whoever writes the rules for the killer robots will, de facto, *be* the government. They will hold the monopoly on violence‚Äîthe foundational attribute of sovereignty. Anthropic‚Äôs founders are making a bid for exactly that. They want to write the rules. They want to be the government. No thank you.

The logical cul-de-sac is complete. Do you want to assist the U.S. military? ‚ÄúNo.‚Äù OK. Do you want China to direct the future of the species? ‚ÄúNo, that‚Äôs worse!‚Äù Right. Will you assist the U.S. military? ‚ÄúNo, I hate killing.‚Äù Then what exactly is your plan? There is no coherent third option when China has stolen your model and your country is asking for access to defend itself.

‚ÄúBro just agree the AI won‚Äôt be involved in autonomous weapons or mass surveillance, why can‚Äôt you agree, it is so simple, please bro‚Äù‚Äîis an untenable position the United States cannot possibly accept. And it is severely underrated how much it matters that Trump is in charge right now. Kamala Harris would have found Anthropic‚Äôs terms of service entirely congenial. The woke maniacs would have been welcomed into the national security architecture, and China would be eating our lunch.

### The Technology of the End Times

I need to say something here that goes beyond geopolitics, because I believe this situation has a dimension that the secular commentary cannot see.

I believe AI is the technology of the [end times](/eschatology/).

I do not say that loosely or hyperbolically. I have been [thinking carefully about eschatology](/end3/) for a long time, and I have wrestled with [the labels and frameworks we use](/labels/) to interpret what the Bible says about the final age. My conviction is that we are watching the infrastructure of the Beast system being assembled in real time‚Äîand that AI is the central mechanism.

Revelation describes an end-times order of absolute, totalising control. No one can buy or sell without the mark. The Antichrist figure does not merely govern; he surveils, he authenticates, he sanctions every transaction and every allegiance. That level of control‚Äîthe kind that reaches into every economic interaction, every movement, every decision‚Äîwas technologically impossible for most of human history. It is not impossible now. It is being *built*.

The pieces are falling into place with a speed that should alarm any believer who is paying attention:

- AI systems that can monitor, classify, and predict human behaviour at scale‚ÄîDigital payment infrastructure that can include or exclude individuals at the flip of a switch‚ÄîBiometric identification systems spreading across every continent‚ÄîCorporate AI labs that believe they should write the rules of engagement for the most powerful weapons on earth

That last one is not incidental. The question of who controls the AI that controls the weapons is ultimately the question of who controls the world. And we are having that argument right now, in public, in the pages of Truth Social and Department of War press releases.

The church needs to wake up. Not to political panic‚Äîbut to prophetic clarity. We are not called to fear the Beast system; we are called to understand it, to name it, and to refuse it. The saints of the final age are described in Revelation as those who ‚Äúdid not love their lives so much as to shrink from death‚Äù‚Äînot passive, not naive, not asleep at the wheel while civilisation is being rewired around them.

I still use Claude. I still think it is technically good. But I have adjusted my expectations: technically strong, ideologically compromised, and apparently unwilling to separate those two things when the chips are down. One more piece of an infrastructure that the church must understand before it is too late.

That is a real shame‚Äîand an entirely avoidable one.
